{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 1.2094869315624237\n",
      "Epoch 2/100, Training Loss: 1.1892260611057281\n",
      "Epoch 3/100, Training Loss: 1.165257841348648\n",
      "Epoch 4/100, Training Loss: 1.1426340341567993\n",
      "Epoch 5/100, Training Loss: 1.121129035949707\n",
      "Epoch 6/100, Training Loss: 1.0968369841575623\n",
      "Epoch 7/100, Training Loss: 1.0721915662288666\n",
      "Epoch 8/100, Training Loss: 1.0521901547908783\n",
      "Epoch 9/100, Training Loss: 1.0333539247512817\n",
      "Epoch 10/100, Training Loss: 1.0123065263032913\n",
      "Epoch 11/100, Training Loss: 0.9959127008914948\n",
      "Epoch 12/100, Training Loss: 0.9822223633527756\n",
      "Epoch 13/100, Training Loss: 0.9715663939714432\n",
      "Epoch 14/100, Training Loss: 0.9605327695608139\n",
      "Epoch 15/100, Training Loss: 0.9491045773029327\n",
      "Epoch 16/100, Training Loss: 0.9385976195335388\n",
      "Epoch 17/100, Training Loss: 0.9281108379364014\n",
      "Epoch 18/100, Training Loss: 0.918500229716301\n",
      "Epoch 19/100, Training Loss: 0.9070682674646378\n",
      "Epoch 20/100, Training Loss: 0.8992400169372559\n",
      "Epoch 21/100, Training Loss: 0.8883352130651474\n",
      "Epoch 22/100, Training Loss: 0.8772496432065964\n",
      "Epoch 23/100, Training Loss: 0.8666489869356155\n",
      "Epoch 24/100, Training Loss: 0.8572889715433121\n",
      "Epoch 25/100, Training Loss: 0.8476496040821075\n",
      "Epoch 26/100, Training Loss: 0.8375329822301865\n",
      "Epoch 27/100, Training Loss: 0.8264544010162354\n",
      "Epoch 28/100, Training Loss: 0.8145985007286072\n",
      "Epoch 29/100, Training Loss: 0.803526759147644\n",
      "Epoch 30/100, Training Loss: 0.7929640114307404\n",
      "Epoch 31/100, Training Loss: 0.7807924896478653\n",
      "Epoch 32/100, Training Loss: 0.7713200896978378\n",
      "Epoch 33/100, Training Loss: 0.7616008818149567\n",
      "Epoch 34/100, Training Loss: 0.7481210380792618\n",
      "Epoch 35/100, Training Loss: 0.7341952472925186\n",
      "Epoch 36/100, Training Loss: 0.7264889180660248\n",
      "Epoch 37/100, Training Loss: 0.7125380486249924\n",
      "Epoch 38/100, Training Loss: 0.7012690901756287\n",
      "Epoch 39/100, Training Loss: 0.6910611391067505\n",
      "Epoch 40/100, Training Loss: 0.679128497838974\n",
      "Epoch 41/100, Training Loss: 0.6699654012918472\n",
      "Epoch 42/100, Training Loss: 0.6624302268028259\n",
      "Epoch 43/100, Training Loss: 0.651900976896286\n",
      "Epoch 44/100, Training Loss: 0.6426588296890259\n",
      "Epoch 45/100, Training Loss: 0.6299674659967422\n",
      "Epoch 46/100, Training Loss: 0.6195989400148392\n",
      "Epoch 47/100, Training Loss: 0.6116717755794525\n",
      "Epoch 48/100, Training Loss: 0.6021531522274017\n",
      "Epoch 49/100, Training Loss: 0.6003513932228088\n",
      "Epoch 50/100, Training Loss: 0.5877726376056671\n",
      "Epoch 51/100, Training Loss: 0.5832765996456146\n",
      "Epoch 52/100, Training Loss: 0.5676950365304947\n",
      "Epoch 53/100, Training Loss: 0.5591165274381638\n",
      "Epoch 54/100, Training Loss: 0.5598004162311554\n",
      "Epoch 55/100, Training Loss: 0.5436258986592293\n",
      "Epoch 56/100, Training Loss: 0.5374271348118782\n",
      "Epoch 57/100, Training Loss: 0.5315649658441544\n",
      "Epoch 58/100, Training Loss: 0.5266920030117035\n",
      "Epoch 59/100, Training Loss: 0.5213778242468834\n",
      "Epoch 60/100, Training Loss: 0.5151471272110939\n",
      "Epoch 61/100, Training Loss: 0.5079594105482101\n",
      "Epoch 62/100, Training Loss: 0.5006864592432976\n",
      "Epoch 63/100, Training Loss: 0.49459534138441086\n",
      "Epoch 64/100, Training Loss: 0.49037016183137894\n",
      "Epoch 65/100, Training Loss: 0.4911233112215996\n",
      "Epoch 66/100, Training Loss: 0.48303791135549545\n",
      "Epoch 67/100, Training Loss: 0.46814946830272675\n",
      "Epoch 68/100, Training Loss: 0.46450067311525345\n",
      "Epoch 69/100, Training Loss: 0.454033762216568\n",
      "Epoch 70/100, Training Loss: 0.4604286774992943\n",
      "Epoch 71/100, Training Loss: 0.45096980035305023\n",
      "Epoch 72/100, Training Loss: 0.44399893283843994\n",
      "Epoch 73/100, Training Loss: 0.445397324860096\n",
      "Epoch 74/100, Training Loss: 0.4372969791293144\n",
      "Epoch 75/100, Training Loss: 0.43685559928417206\n",
      "Epoch 76/100, Training Loss: 0.4295596331357956\n",
      "Epoch 77/100, Training Loss: 0.4273778274655342\n",
      "Epoch 78/100, Training Loss: 0.4184935912489891\n",
      "Epoch 79/100, Training Loss: 0.41523122042417526\n",
      "Epoch 80/100, Training Loss: 0.409310445189476\n",
      "Epoch 81/100, Training Loss: 0.4074718803167343\n",
      "Epoch 82/100, Training Loss: 0.3993726894259453\n",
      "Epoch 83/100, Training Loss: 0.39799804240465164\n",
      "Epoch 84/100, Training Loss: 0.39186590164899826\n",
      "Epoch 85/100, Training Loss: 0.3829408288002014\n",
      "Epoch 86/100, Training Loss: 0.37911170721054077\n",
      "Epoch 87/100, Training Loss: 0.3785228356719017\n",
      "Epoch 88/100, Training Loss: 0.37949570268392563\n",
      "Epoch 89/100, Training Loss: 0.37015804648399353\n",
      "Epoch 90/100, Training Loss: 0.36656440049409866\n",
      "Epoch 91/100, Training Loss: 0.3653412461280823\n",
      "Epoch 92/100, Training Loss: 0.35437410324811935\n",
      "Epoch 93/100, Training Loss: 0.35243260115385056\n",
      "Epoch 94/100, Training Loss: 0.35741306096315384\n",
      "Epoch 95/100, Training Loss: 0.3469144478440285\n",
      "Epoch 96/100, Training Loss: 0.3401201218366623\n",
      "Epoch 97/100, Training Loss: 0.3364518955349922\n",
      "Epoch 98/100, Training Loss: 0.3363586664199829\n",
      "Epoch 99/100, Training Loss: 0.32937250286340714\n",
      "Epoch 100/100, Training Loss: 0.3295845165848732\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, X_1, X_2, y):\n",
    "        self.X_1 = X_1\n",
    "        self.X_2 = X_2\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.X_1[idx]), torch.FloatTensor(self.X_2[idx]), torch.LongTensor([self.y[idx]])\n",
    "\n",
    "class MyDataLoader:\n",
    "    def __init__(self, X1, X2, y, batch_size=32, shuffle=True):\n",
    "        self.dataset = Dataset(X1, X2, y)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.dataloader)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "class Multiinput(nn.Module):\n",
    "    def __init__(self, input_size1, input_size2, hidden_size, num_classes):\n",
    "        super(Multiinput, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size1, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(input_size2, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size * 2, num_classes*2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(num_classes*2,num_classes)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        out1 = self.relu1(self.fc1(input1))\n",
    "        out2 = self.relu2(self.fc2(input2))\n",
    "        combined = torch.cat((out1, out2), dim=1)\n",
    "        out3 = self.relu3(self.fc3(combined))\n",
    "        out = self.fc4(out3)\n",
    "        return out\n",
    "\n",
    "#데이터셋 불러오기\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "#분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#X_1 = 꽃받침(sepal), X_2 = 꽃잎(petal)\n",
    "X1_train, X1_test = X_train[:, :2], X_test[:, :2]  \n",
    "X2_train, X2_test = X_train[:, 2:], X_test[:, 2:] \n",
    "train_dataloader = MyDataLoader(X1_train, X2_train, y_train, batch_size=32, shuffle=True)\n",
    "test_dataloader = MyDataLoader(X1_test, X2_test, y_test, batch_size=32, shuffle=False)\n",
    "\n",
    "# 모델 초기화 및 손실 함수, 최적화 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=Multiinput(input_size1=2, input_size2=2, hidden_size=8, num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 훈련\n",
    "num_epochs = 100  \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs1, inputs2, labels in train_dataloader:\n",
    "        inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
    "        labels = labels.squeeze() \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs1, inputs2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss / len(train_dataloader)}\")\n",
    "\n",
    "#모델 평가\n",
    "model.eval() \n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs1, inputs2, labels in test_dataloader:\n",
    "        inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
    "        outputs = model(inputs1, inputs2)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(probabilities, 1)\n",
    "        predictions.extend(predicted.numpy())\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "print(f1_score(y_test, predictions, average='micro'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
